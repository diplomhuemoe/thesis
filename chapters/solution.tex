% -*-coding: utf-8-*-
\chapter{Исследование и построение решения задачи}

Основной задачей, поставленной в данной работе является разработка метода
построения словаря эмоционально окрашенных слов на основе алгоритма
распространяющейся активации. Соответственно решение данной задачи включает в
себя следующие пункты:

\begin{enumerate}
  \item Теоретическое исследование применимости метода распространяющейся
    активации для задачи построения словаря эмоционально окрашенных слов;
  \item Реализация метода распространяющейся активации и его вариантов;
  \item Экспериментальное сравнение реализованных алгоритмов.
\end{enumerate} 

\section{Теоретическое исследование применимости метода распространяющейся
  активации}
\subsection{Алгоритм распространяющейся активации}
Пусть задан граф с $N$ вершинами, каждой из которых сопоставлено значение
активации $A[i]$, которое является вещественным числом на отрезке $[0; 1]$.
Каждое ребро $[i, j]$, соединяющее вершины $i$ и $j$ имеет вес, обозначаемый как
$W[i, j]$, который тоже является вещественным числом с отрезка $[0; 1]$. Также
задан коэффициент затухания D, имеющий вещественное значение с отрезка $[0; 1]$.

Процедура:
\begin{enumerate}
\item Все значения активации $[i]$ обнуляются. Выбирается несколько вершин, с
  которых начнется активация и устанавливается их $A[i]$;
\item Для каждой вершины, имеющей ненулевое $A[i]$, рассчитывается значение
  активации всех его соседних вершин(которые соединены с ним ребром $[i, j]$) по
  следующей формуле $A[j] = max(A[j]; A[i] * W[i, j] * D)$;
\item Пункт 2 повторяется до тех пор пока суммарное значение изменения активации
  по всем вершинам не станет меньше какого то установленного малого значения.
\end{enumerate}

\subsection{Применение метода распространяющейся активации для задачи построения
  словаря эмоционально окрашенных слов}
Для построения необходимого графа слова выступают как вершины, а наличие
ребра между словами означает существования биграммы из них в текстовом корпусе.
Каждая вершина при этом уникальна и в итоге получается граф всех словосочетаний,
считанных из набора данных. Веса ребер для каждой биграммы также заданы
как какая-либо её характеристика. Например, это может быть количество вхождения
биграммы в текст.

Для каждой вершины в графе будет два значения активации: первое (будем его
называть $P[i]$) является вероятностью принадлежности слова к классу позитивно
окрашенных, второе значение (назовем его $N[i]$) - это вероятность принадлежности
к классу негативно окрашенных слов. Для инициализации вручную составляется
два списка слов. В первом списке слова придающие тексту заведомо позитивную
окраску, поэтому их $P[i]$ устанавливается равным единице при инициализации графа.
Во втором списке слова придающие негативную окраску, их $N[i]$ становятся равны
также единице. Оба списка представлены в Приложении $1$. После этого на графе
запускается распространяющаяся активация.

\section{Реализация метода распространяющейся активации и его вариантов}
\subsection{Использование национального корпуса русского языка}
Первым был выбран текстовый корпус ruscorpora, который составлен, по большей
части, из литературы XX века. Для создания графа были использованы частоты
вхождения каждой биграммы в весь корпус, предоставляемые создателями этой
коллекции документов.

Теперь $W[i, j]$ - это число вхождения биграммы в корпус и оно, соответственно,
больше $1$. Но $A[j]$ это вероятность быть отнесенным к определенному классу
тональности, а так как $W[i, j]$ теперь может быть довольно большим числом, то
$A[j]$ может стать больше единицы, что недопустимо. Поэтому было принято решение
нормировать веса ребер, исходящих из вершины, на наибольший из них.

Значение активации рассчитывается по следующей модифицированной формуле:
$A[j] = max(A[j]; A[i] * \frac{W[i,j]}{max_i(W[i,j])} * D)$, где максимум
берется по всем $i$ при фиксированном $j$. Соответственно на этом наборе данных
запускался алгоритм распространяющейся активации и на выходе получался набор
данных из слов и сопоставленных каждому из них двух значений: вероятность внести
позитвный вклад в текст и вероятность внести негативный вклад в текст.

\subsection{Технология Word2Vec}
Word2Vec -- технология статистической обработки больших массиво текстовой
информации. Она делает отображение текстового корпуса на множество векторных
представлений слов из этого корпуса. Такие векторные представления основаны
на том как часто слова встречаются вместе или при каких схожих обстоятельствах
они употребляются.

Основу Word2Vec составляют два алгоритма: continuous bag-of-words и skipgram.
Continuous bag-of-words -- это модель в которой слова из предложения или
текста рассматриваются как одно большое множество, игнорируя порядок в котором
они следовали друг за другом. Skip-gram -- это обобщение понятия N-граммы, в
котором слова из рассматриваемого текста не обязаны браться последовательно, а
могут пропускаться.

По сути, два этих алгоритма можно рассматривать как выходной слой в нейронной
сети, которая используется для обучения Word2Vec. Притом continuous bagof-words
используется для предсказания слова по контексту, который подается на
входной слой, а skip-gram используется для предсказания контекста по слову,
поданному на вход.

Чтобы получить качественные зависимости между векторными представлениями слов,
нужно обучать нейронную сеть на больших текстовых корпусах. В данной
работе используется Word2Vec, обученный на русской Википедии.

Поскольку каждое слово представимо в виде численного вектора, есть возможность
считать косинус угла между ними. Соответственно модуль этой величины всегда
меньше единицы и в дальнейшем она будет называться сходством между словами.
Чем она больше, тем более синонимичны рассматриваемые два слова между собой.

\subsection{Создание словаря c использованием Word2Vec}
На этом этапе было необходимо осуществить предварительную обработку векторных
представлений всех слов, встречаемых в русской Википедии. Из них составлялись
биграммы по следующему принципу: для каждого слова находилось 50 наиболее схожих
с ним, и соответственно числом, характеризующим биграмму, являлось
сходство между её элементами.

Теперь в методе распространяющейся активации $W[i, j]$ это сходство между сло-
вами, и, так как оно по модулю не превышает единицу, $A[i]$ будет лежать в пределах
$[0;1]$. Таким образом, значение активации можно рассчитывать следующим образом:
$A[j] = max(A[j]; A[i] * W[i, j] * D)$. Тем не менее тестировался и вариант с нормировкой
со следующей формулой: $A[j] = max(A[j]; A[i] * \frac{W[i,j]}{max_i(W[i,j])} * D)$.

В результате обработки также получился словарь, состоящий из слов русского
языка и соответствующих каждому из них два значения: его позитивная оценка и
негативная.

\subsection{Модификация метода на машинном обучении}
Как было описано в обзоре существующих решений, задачи анализа тональности
текста можно решать методами машинного обучения. А полученный словарь можно
использовать для модификации признаков, используемых для обучения
классификатора.

Изначально векторами признаков являются числа вхождений каждой униграммы из
корпуса в элементы выборки. То есть, признаков столько же, сколько и униграмм во
всем текстовом корпусе. В качестве классификатора использовался наивный
байесовский классификатор.

Модификация этого метода словарем заключается в следующем: для текстового
корпуса, на котором будет обучаться классификатор, рассчитывается тональность
каждого элемента корпуса с помощью полученного словаря. И полученные значения
тональности можно использовать как еще один дополнительный вектор признаков
для обучения классификатора.

\subsection{Экспериментальное сравнение реализованных алгоритмов}
Полученные в результате работы алгоритма словари были протестированы на
корпусе сообщений Twitter ИСП РАН. Он состоит из 6328 вручную размеченных на
три категории тональности твитов: положительные, нейтральные и негативные. В
Таб. 1 приведено описание корпуса:

\todo{Табличка}

\FloatBarrier

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
